---
title: Serve
wip: false
---

## Serve machine-learning models

In the previous section we have introduced the command **run** of <FlamaName /> CLI. The **run** command allowed us to
serve any <FlamaName /> app, being possible to adapt the configuration of the app using the standard **uvicorn**
arguments. But this is not the end of the story.

The command **serve** comes to the rescue of those who are looking for an instantaneous serving of an ML model without
having to write an app. This command, as we are about to see, only requires the ML model to be served. The model will
have to be saved as a binary file beforehand by using the tools offered by <FlamaName />. The details about how to
properly save the models can be found in the section [packaging models](/docs/machine-learning-api/packaging-models).

### Example files

The best way of seeing the power of **serve** is by example. Although we have not yet seen how to produce the ML file
needed (go to [packaging models](/docs/machine-learning-api/packaging-models) to learn how), we can use any of the
following example files for the sake of learning:

- [Scikit Learn model](/models/sklearn_model.flm)
- [TensorFlow model](/models/tensorflow_model.flm)
- [PyTorch model](/models/pytorch_model.flm)

Once you have downloaded any of the files listed above, you will be able to serve your first ML model codeless.

## Codeless serving

Let's pick up one of the example model files we have just introduced in the previous section, say **sklearn_model.flm**.
Now, let's run:

```commandline
> flama serve sklearn_model.flm

INFO:     Started server process [15822]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

Does it seem familiar? Yup, indeed, as you can see we have a <FlamaName /> app up and running on **http://127.0.0.1**
and listening via the port **8000**. But, this time we have not written a single line of code! This is the **codeless
approach** of <FlamaName /> to ML models deployment, which makes possible the deployment of an already trained-tested ML
model as an API ready to receive requests and return estimations almost without any effort.

### How do we interact with the model?

So, we have an app which is running an ML model. The natural question to ask now is, how do we interact with that model?
All models served with **flama serve** come with the following endpoints for free:

- <Label color="green">GET</Label> **http://127.0.0.1:8000/docs/**: Returns an interactive HTML documentation which allows
  us to interact with the model in our web browser. This is the recommended way to start interacting with the model, as it
  informs us about all the routes available, and helps us to figure how to programmatically interact with all other routes.
- <Label color="green">GET</Label> **http://127.0.0.1:8000/schema/**: Returns a JSON which represents the model being served.
- <Label color="blue">POST</Label> **http://127.0.0.1:8000/predict/** Expects the input data (observations) to be passed
  as argument to predict their output.

### Interactive and intuitive documentation

As mentioned in the previous section, we recommend inspecting the documentation route (`/docs/`) as soon as we have the
model serving. To do that you only need to **flama serve** whatever model which you have packaged previously, and you'll
get something like:

<Image src="/images/docs/docs-schema-get.png" alt="docs-get-schema" width="1600" height="400" />

As you can see, the documentation page not only gives you a handy way to interact with the different endpoints
available, but also gives you the equivalent `curl` statement. For instance, in order to get the schema of the model:

```commandLine
$ curl --request GET \
  --url http://127.0.0.1:8000/ \
  --header 'Content-Type: application/json'

{
  "C": 1,
  "class_weight": null,
  "dual": false,
  "fit_intercept": true,
  "intercept_scaling": 1,
  "l1_ratio": null,
  "max_iter": 100,
  "multi_class": "auto",
  "n_jobs": null,
  "penalty": "l2",
  "random_state": null,
  "solver": "lbfgs",
  "tol": 0.0001,
  "verbose": 0,
  "warm_start": false
}
```

Or, in case we want to make a prediction:

<Image src="/images/docs/docs-predict-post.png" alt="docs-post-predict" width="1600" height="760" />

```commandLine
curl --request POST \
  --url http://127.0.0.1:8000/predict/ \
  --header 'Content-Type: application/json' \
  --data '{"input": [[1,2,3], [1,2,3]]}'

{
  "output": [
    0,
    0
  ]
}
```

ðŸ¥³ Awesome! Now, not only you have your model serving, but you also made predictions by interacting with it
programmatically. The cool thing about this, is that this way of interacting with your model via _requests_ is the same,
no matter where the model is deployed. And, this is a huge step towards the productionalization of your model. Now, we
have a reproducible problem, which is key to understand and replicate the behaviour of our models in a production
environment, since it'll be the same as we observe in local.

## Parameters

Although the default usage of **flama serve** is already very convenient, you might be interested in customise a bit the
result. For instance, you might want to serve the model at a different route (i.e., not at the default **/**), or the
name of the model (i.e., not **model**), and so on. Such a customisation is possible thanks to the different optional
inputs of the CLI:

```commandline
> flama serve --help

Usage: flama serve [OPTIONS] MODEL_PATH

  Serve the ML model file at <MODEL_PATH> within a Flama Application.

Options:
  --dev                           Development mode.
  --model-url TEXT                Route of the model  [default: /]
  --model-name TEXT               Name of the model  [default: model]
  --app-title TEXT                Name of the application  [default: Flama]
  --app-version TEXT              Version of the application  [default: 0.1.0]
  --app-description TEXT          Description of the application  [default:
                                  Fire up with the flame]
  --app-schema TEXT               Route of the application schema  [default:
                                  /schema/]
  --app-docs TEXT                 Route of the application documentation
                                  [default: /docs/]
  --host TEXT                     Bind socket to this host.  [default:
                                  127.0.0.1]
  --port INTEGER                  Bind socket to this port.  [default: 8000]
  --reload                        Enable auto-reload.
  --uds TEXT                      Bind to a UNIX domain socket.
  --fd INTEGER                    Bind to socket from this file descriptor.
  --reload-dir PATH               Set reload directories explicitly, instead
                                  of using the current working directory.
  --reload-include TEXT           Set glob patterns to include while watching
                                  for files. Includes '*.py' by default; these
                                  defaults can be overridden with `--reload-
                                  exclude`. This option has no effect unless
                                  watchfiles is installed.
  --reload-exclude TEXT           Set glob patterns to exclude while watching
                                  for files. Includes '.*, .py[cod], .sw.*,
                                  ~*' by default; these defaults can be
                                  overridden with `--reload-include`. This
                                  option has no effect unless watchfiles is
                                  installed.
  --reload-delay FLOAT            Delay between previous and next check if
                                  application needs to be. Defaults to 0.25s.
                                  [default: 0.25]
  --workers INTEGER               Number of worker processes. Defaults to the
                                  $WEB_CONCURRENCY environment variable if
                                  available, or 1. Not valid with --dev.
  --loop [auto|asyncio|uvloop]    Event loop implementation.  [default: auto]
  --http [auto|h11|httptools]     HTTP protocol implementation.  [default:
                                  auto]
  --ws [auto|none|websockets|wsproto]
                                  WebSocket protocol implementation.
                                  [default: auto]
  --ws-max-size INTEGER           WebSocket max size message in bytes
                                  [default: 16777216]
  --ws-ping-interval FLOAT        WebSocket ping interval  [default: 20.0]
  --ws-ping-timeout FLOAT         WebSocket ping timeout  [default: 20.0]
  --ws-per-message-deflate BOOLEAN
                                  WebSocket per-message-deflate compression
                                  [default: True]
  --lifespan [auto|on|off]        Lifespan implementation.  [default: auto]
  --interface [auto|asgi3|asgi2|wsgi]
                                  Select ASGI3, ASGI2, or WSGI as the
                                  application interface.  [default: auto]
  --env-file PATH                 Environment configuration file.
  --log-config PATH               Logging configuration file. Supported
                                  formats: .ini, .json, .yaml.
  --log-level [critical|error|warning|info|debug|trace]
                                  Log level. [default: info]
  --access-log / --no-access-log  Enable/Disable access log.
  --use-colors / --no-use-colors  Enable/Disable colorized logging.
  --proxy-headers / --no-proxy-headers
                                  Enable/Disable X-Forwarded-Proto,
                                  X-Forwarded-For, X-Forwarded-Port to
                                  populate remote address info.
  --server-header / --no-server-header
                                  Enable/Disable default Server header.
  --date-header / --no-date-header
                                  Enable/Disable default Date header.
  --forwarded-allow-ips TEXT      Comma separated list of IPs to trust with
                                  proxy headers. Defaults to the
                                  $FORWARDED_ALLOW_IPS environment variable if
                                  available, or '127.0.0.1'.
  --root-path TEXT                Set the ASGI 'root_path' for applications
                                  submounted below a given URL path.
  --limit-concurrency INTEGER     Maximum number of concurrent connections or
                                  tasks to allow, before issuing HTTP 503
                                  responses.
  --backlog INTEGER               Maximum number of connections to hold in
                                  backlog
  --limit-max-requests INTEGER    Maximum number of requests to service before
                                  terminating the process.
  --timeout-keep-alive INTEGER    Close Keep-Alive connections if no new data
                                  is received within this timeout.  [default:
                                  5]
  --ssl-keyfile TEXT              SSL key file
  --ssl-certfile TEXT             SSL certificate file
  --ssl-keyfile-password TEXT     SSL keyfile password
  --ssl-version INTEGER           SSL version to use (see stdlib ssl module's)
                                  [default: 17]
  --ssl-cert-reqs INTEGER         Whether client certificate is required (see
                                  stdlib ssl module's)  [default: 0]
  --ssl-ca-certs TEXT             CA certificates file
  --ssl-ciphers TEXT              Ciphers to use (see stdlib ssl module's)
                                  [default: TLSv1]
  --header TEXT                   Specify custom default HTTP response headers
                                  as a Name:Value pair
  --app-dir TEXT                  Look for APP in the specified directory, by
                                  adding this to the PYTHONPATH. Defaults to
                                  the current working directory.  [default: .]
  --h11-max-incomplete-event-size INTEGER
                                  For h11, the maximum number of bytes to
                                  buffer of an incomplete event.
  --factory                       Treat APP as an application factory, i.e. a
                                  () -> <ASGI app> callable.
  --help                          Show this message and exit.

```

We can readily see the following groups of parameters:

### Model parameters

- **model-url**: Route of the model (default: /)
- **model-name**: Name of the model (default: model)

### App parameters

- **app-title**: Name of the application (default: Flama)
- **app-version**: Version of the application (default: 0.1.0)
- **app-description**: Description of the application (default: Fire up) with the flame]
- **app-docs**: Description of the application (default: Fire up) with the flame]
- **app-schema**: Description of the application (default: Fire up) with the flame]

### Server parameters

- **host**: Bind socket to this host (default: 127.0.0.1)
- **port**: Bind socket to this port (default: 8000)
- All other **uvicorn** options

Each of these options can be passed directly to the CLI by using `--<option-name> <value>`, or by utilising the
equivalent environment variable, i.e. `FLAMA_<OPTION_NAME>=<VALUE>` (take care, the equivalent environment variable
needs to have the prefix **FLAMA**, and written in capital letters, and underscored). For instance, if we want to change
the route where the model is served we can do it by any of the following two equivalent ways:

#### Using option argument

```commandline
> flama serve sklearn_model.flm --model-url=/logreg
```

#### Using environment variable

```commandline
> export FLAMA_MODEL_PATH=sklearn_model.flm
> export FLAMA_MODEL_URL=/logreg
> flama serve sklearn_model.flm
```

As a complete example:

```commanline
> flama serve sklearn_model.flm \
    --model-url="/logreg" \
    --model-name="Logistic Regression" \
    --app-title="ML API" \
    --app-version="1.0.0" \
    --app-description="Logistic Regression model serving for predictions"
```

With these changes, the documentation at **http://http://127.0.0.1:8000/docs/** will look like:

<Image src="/images/docs/docs-serve-params.png" alt="docs-post-predict" width="1600" height="912" />
