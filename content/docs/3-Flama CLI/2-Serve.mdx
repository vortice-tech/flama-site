---
title: Serve
wip: true
---

import FlamaName from '@/components/FlamaName'
import PythonName from '@/components/PythonName'

## Serve machine-learning models

In the previous section we have introduced the command **run** of <FlamaName /> CLI.
The **run** command allowed us to serve any <FlamaName /> app, being possible to adapt the configuration
of the app using the standard **uvicorn** arguments. But this is not the end of the story.

The command **serve** comes to the rescue of those who are looking for an instantaneous serving of
an ML model without having to write an app. This command, as we are about to see, only
requires the ML model to be served. The model will have be saved as a binary file beforehand using
the tools offered by <FlamaName />. The details about how to save the models properly can be
found in the section [packaging models](/docs/machine-learning-api/packaging-models).

### Example

The best way of seeing the power of **serve** is by example. Although we have not seen yet how
to produce the ML file needed (go to [packaging models](/docs/machine-learning-api/packaging-models) to learn
how), we can use any of the following examples:

- [Scikit Learn model](/models/sklearn_model.flm)
- [TensorFlow model](/models/tensorflow_model.flm)
- [PyTorch model](/models/pytorch_model.flm)

Once you have downloaded any of the files, you are ready to serve your first ML model codeless.
For instance:

```commandline
> flama serve sklearn_model.flm

INFO:     Started server process [15822]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

As you can see, this time we have not written a single line of code.
This is the codeless approach of <FlamaName /> to ML models deployment,
which makes possible the deployment of an already trained-tested ML model as an API
ready to receive requests and return estimations almost without any effort.

Here we present the endpoints and documentation (SWAGER) auto-geneated, besides any other optional flag we might
be able to pass to uvicorn, either as parameter, option or ENV. Examples:

- FLAMA_APP
- FLAMA_MODEL
- FLAMA_URL
