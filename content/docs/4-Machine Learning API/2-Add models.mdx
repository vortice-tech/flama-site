---
title: Add models to your App
wip: true
---

## Introduction

In this section we are going to introduce a new way of using FLM files, which will be useful for those who are
interested in developing their own <FlamaName/> App. There are several reasons why you can consider developing your own
app, e.g. amongst other reasons you might want:

- To add further functionality which is not built-in when using the <FlamaName/> CLI
- To gain control on fine details for technical reasons
- To learn by doing

This is perfectly okay, and normal. <FlamaName/> CLI is not a "one size fits all" answer. Indeed, there are some
characteristics of a <FlamaName/> App which we may need to customise, for which we need to develop the application
ourselves, e.g. **on-startup** or **on-shutdown** events, to name some examples.

## Adding models

Once you have your ML models packaged in _flm_ format, they can be exposed as
[Resources](/docs/machine-learning-api/model-resource) of a Flama App. This is what constitutes what we refer to as a
_Flama ML API_.

### PyTorch

```python
import typing

from flama import Flama

app = Flama()

app.models.add_model(
    path="/torch_model",
    model="path/to/your_model_file.flm",
    name="name_of_the_torch_model",
)
```

- **GET /torch_model**: Returns _resource_, i.e. the schema with `get_params()`
- **POST /torch_model/predict**: Returns prediction of model

### Scikit-Learn

```python
import typing

from flama import Flama

app = Flama()

app.models.add_model(
    path="/sk_model",
    model="path/to/your_model_file.flm",
    name="name_of_the_sk_model",
)
```

- **GET /sk_model**: Returns _resource_, i.e. the schema with `get_params()`
- **POST /sk_model/predict**: Returns prediction of model

### TensorFlow

```python
import typing

from flama import Flama

app = Flama()

app.models.add_model(
    path="/tf_model",
    model="path/to/your_model_file.flm",
    name="name_of_the_tf_model",
)
```

If you run this code, you will get an API with the following endpoints:

- **GET /tf_model**: Returns _resource_, i.e. the schema with `get_params()`
- **POST /tf_model/predict**: Returns prediction of model

### Multiple models

```python
import typing

from flama import Flama

app = Flama()

app.models.add_model(
    path="/torch_model",
    model="path/to/your_model_file.flm",
    name="name_of_the_torch_model",
)

app.models.add_model(
    path="/sk_model",
    model="path/to/your_model_file.flm",
    name="name_of_the_sk_model",
)

app.models.add_model(
    path="/tf_model",
    model="path/to/your_model_file.flm",
    name="name_of_the_tf_model",
)
```

- **GET /tf_model**: Returns _resource_, i.e. the schema with `get_params()`
- **POST /tf_model/predict**: Returns prediction of model
- **GET /sk_model**: Returns _resource_, i.e. the schema with `get_params()`
- **POST /sk_model/predict**: Returns prediction of model
- **GET /torch_model**: Returns _resource_, i.e. the schema with `get_params()`
- **POST /torch_model/predict**: Returns prediction of model

**Remark**: Show SWAGGER docs for all combinations
